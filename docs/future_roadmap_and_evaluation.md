# Deep Evaluation & Future Roadmap (v1.6)
*Last updated: 2026-03-01 â€” reflects v1.6 90-fold walk-forward run (2007â€“2026)*

## 1. Deep Evaluation of Current Performance

### Overall Metrics (Full 90-Fold Run: 2007â€“2026)
*   **Mean Rank Accuracy**: **57.6%** (Baseline: 50%)
    *   Statistically robust over 19 years of OOS data including GFC and COVID-19.
*   **Mean Val Loss**: **0.460 Â± 0.017**
*   **Mean Magnitude MAE**: **0.351** â€” stable across all 90 folds.
*   **L/S Alpha**: Positive in **86/90 folds** (95.6%), mean +0.82 per snapshot.
*   **NaN Training Folds**: 10/90 (11%) â€” concentrated in GFC and COVID regimes.
*   **ECE (raw)**: 0.015 â€” near-perfect confidence calibration.
*   **Conformal Coverage**: 89.4% (target â‰¥ 90% for Î±=0.10).

### Regime Analysis (v1.6 Classification)
| Regime | Vol Range | Approx. Folds | Mean Rank Acc. | Assessment |
| :--- | :--- | :--- | :--- | :--- |
| **Calm** | < 0.20 | ~25% of folds | ~58% | **Stable**. Clear trends, orderly sector rotation. |
| **Normal** | 0.20â€“0.50 | ~50% of folds | ~57.5% | **Consistent**. Adaptive Î» and base patience work well. |
| **Crisis** | > 0.50 | ~25% of folds | ~55% | **Degraded but functional**. Extended patience helps. |

### Generalization Gap
*   **Train Loss**: ~0.463 (mean where non-NaN)
*   **Val Loss**: ~0.460
*   **Gap**: ~0.003 (negligible)
*   **Conclusion**: The model is **not overfitting**. Masked Superset Architecture and dynamic early stopping work effectively.

---

## 2. v1.6 Recommendations â€” Status

| Rec | Description | Status |
|-----|-------------|--------|
| 1 | Gradient clipping + AMP fix | âœ… Implemented |
| 2 | Confidence calibration (Temperature/Platt) | âœ… Implemented |
| 3 | Dynamic early stopping patience | âœ… Implemented (recalibrated thresholds) |
| 4 | Adaptive magnitude weight Î» | âœ… Implemented (recalibrated thresholds) |
| 5 | MC Dropout median rank | âœ… Implemented |
| 6 | Conformal prediction sets | âœ… Implemented |
| 7 | Regime classification | âœ… Implemented (recalibrated thresholds) |
| 8 | Long-short alpha tracking | âœ… Implemented |
| 9 | Walk-forward architecture | âœ… Already in v1.5 |
| 10 | Sector-neutral loss | âœ… Already in v1.5 |
| 11 | Benchmark vs simple models | ðŸ”² Not yet implemented |

---

## 3. Current Limitations

### A. NaN Training Loss in Crisis Regimes
10/90 folds exhibit NaN training loss â€” all in high-volatility regime transitions (GFC, COVID). v1.6 mitigates this with NaN batch skipping in `train_epoch`, but root cause (FP16 overflow on extreme returns) remains.

### B. Execution & Costs (Not Modeled)
The backtest does not model slippage or commissions.
*   **Risk**: High-turnover daily rebalancing at 57.5% accuracy could be net-negative after fees.
*   **Fix**: Integrate a 5 bps/trade cost model (TCA).

### C. Temperature Scaling Counter-Productive
The model's raw ECE (0.015) is already excellent. Temperature scaling with T=1.50 actually *increases* ECE to 0.029. This is expected â€” the v1.6 training fixes resolved the overconfidence issue at its root. Temperature calibration should only be applied if future model changes degrade raw calibration.

---

## 4. Future Roadmap

### Phase 0: Remaining Deployment Items
1.  **Benchmark vs simple models** (Rec 11) â€” compare against momentum, sector rotation, and random baselines.
2.  **Transaction Cost Analysis** â€” 5 bps/trade cost model integration.
3.  **Full re-run with recalibrated thresholds** â€” validate regime-adaptive features with corrected boundaries.

### Phase 1: Portfolio Construction (Short-Term)
1.  **Mean-Variance Optimizer** â€” feed `dir_logits` + `mag_preds` into a convex optimizer.
2.  **Risk Parity via graph clusters** â€” equal-risk allocation across learned stock clusters.
3.  **Conformal abstention gating** â€” skip trades where `set_size = 2` or `set_size = 0`.

### Phase 2: Alpha Expansion (Medium-Term)
1.  **Alternative data** â€” news/sentiment as new heterogeneous graph node type.
2.  **RL fine-tuning (PPO)** â€” optimize directly for Sharpe Ratio post-supervised-pretraining.
3.  **Multi-horizon targets** â€” simultaneous 1d, 5d, 20d prediction heads.
